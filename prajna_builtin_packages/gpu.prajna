import cuda;

func init(){
    cuda::cuInit(0i32);
}

func setDevice(id: i64){
    cuda::cudaSetDevice(id).tostr();
}

func maxThreadPerBlock(device: i64)->i64{
    var count: i32;
    cuda::cuDeviceGetAttribute(&count, 1i32, device.toi32());
    return count.toi64();
}

func multiProcessorsCount(device: i64)->i64{
    var count: i32;
    cuda::cuDeviceGetAttribute(&count, 16i32, device.toi32());
    return count.toi64();
}

func launchKernel(kernel_function_pointer: i8*, gridShape: Array<i64, 3>, blockShape: Array<i64, 3>, kernel_params: i8**)->i64{
    var re = cuda::cuLaunchKernel(kernel_function_pointer, gridShape[0].tou32(), gridShape[1].tou32(), gridShape[2].tou32(), blockShape[0].tou32(), blockShape[1].tou32(), blockShape[2].tou32(), 0u32, __nullptr<i8>::create(), kernel_params, cast<i8**>(__nullptr<i8>::create()));
    // "launch re: ".print();
    // re.tostr().print();
    // "\n".print();
    return re.toi64();
}

func launchKernelForDim1(kernel_function_pointer: i8*, paramters: i8**){

}

template <Type_, Dim_>
struct Tensor<Type_, Dim_> {
    data : Type_*;
    // shape: Array<i64, Dim_>;
    layout: Layout<Dim_>;
}

template <Type_, Dim_>
implement Tensor<Type_, Dim_> {
    @static
    func create(shape :Array<i64, Dim_>)->Tensor<Type_, Dim_>{
        // var self: Tensor<Type_, Dim_>;
        // self.shape = shape;
        // var size = 1;
        // for i in 0 to Dim_ {
        //     size = size * shape[i];
        // }
        // var bytes = size * sizeof(Type_);
        // // bytes.tostr().print();
        // var re = cuda::cuMemAlloc(cast<i8**>(&self.data), bytes);
        // // "cuMemAlloc".print();
        // // re.tostr().print();
        // return self;

        var self: Tensor<Type_, Dim_>;
        self.layout = Layout<Dim_>::create(shape);

        var bytes = self.layout.length() * sizeof(Type_);
        cuda::cuMemAlloc(cast<i8**>(&self.data), bytes);
        bindings::registerReferenceCount(cast<undef*>(self.data));
        bindings::incReferenceCount(cast<undef*>(self.data));

        return self;
    }

    @property("at", "getter")
    func at_getter(idx: Array<i64, Dim_>)->Type_{
        var offset = this.layout.arrayIndexToLinearIndex(idx);
        return this.data[offset];
    }

    @property("at", "setter")
    func at_setter(idx: Array<i64, Dim_>, value: Type_){
        var offset = this.layout.arrayIndexToLinearIndex(idx);
        this.data[offset] = value;
    }

    @property("[", "getter")
    func index_getter(idx: Array<i64, Dim_>)->Type_{
        return this.at(idx);
    }

    @property("[", "setter")
    func index_setter(idx: Array<i64, Dim_>, value: Type_){
        this.at(idx) = value;
    }

    func toHost()->::Tensor<Type_, Dim_>{
        var host_tensor = ::Tensor<Type_, Dim_>::create(this.layout.shape);
        var size = 1;
        for i in 0 to Dim_ {
            size = size * this.layout.shape[i];
        }
        // print("toHost: size");
        // size.tostr().print();
        // print("re: ");
        var re =  cuda::cudaMemcpy(cast<i8*>(host_tensor.data), cast<i8*>(this.data), size * sizeof(Type_), 2i32);
        // re.tostr().print();
        return host_tensor;
    }
}

template <Type_, Dim_>
implement ::Tensor<Type_, Dim_> {
    func toGpu()->Tensor<Type_, Dim_>{
        var cuda_tensor = Tensor<Type_, Dim_>::create(this.layout.shape);
        var size = 1;
        for i in 0 to Dim_ {
            size = size * this.layout.shape[i];
        }
        // print("tocuda: size");
        // size.tostr().print();
        // print("re: ");
        var re = cuda::cudaMemcpy(cast<i8*>(cuda_tensor.data), cast<i8*>(this.data), size * sizeof(Type_), 1i32);
        // re.tostr().print();
        return cuda_tensor;
    }
}

@target("nvptx")
@intrinsic("llvm.nvvm.read.ptx.sreg.tid.x")
func __thread_idx_x()->u32;

@target("nvptx")
@intrinsic("llvm.nvvm.read.ptx.sreg.tid.y")
func __thread_idx_y()->u32;

@target("nvptx")
@intrinsic("llvm.nvvm.read.ptx.sreg.tid.z")
func __thread_idx_z()->u32;

@target("nvptx")
@intrinsic("llvm.nvvm.read.ptx.sreg.ntid.x")
func __block_shape_x()->u32;

@target("nvptx")
@intrinsic("llvm.nvvm.read.ptx.sreg.ntid.y")
func __block_shape_y()->u32;

@target("nvptx")
@intrinsic("llvm.nvvm.read.ptx.sreg.ntid.z")
func __block_shape_z()->u32;

@target("nvptx")
@intrinsic("llvm.nvvm.read.ptx.sreg.ctaid.x")
func __block_idx_x()->u32;

@target("nvptx")
@intrinsic("llvm.nvvm.read.ptx.sreg.ctaid.y")
func __block_idx_y()->u32;

@target("nvptx")
@intrinsic("llvm.nvvm.read.ptx.sreg.ctaid.z")
func __block_idx_z()->u32;

@target("nvptx")
@intrinsic("llvm.nvvm.read.ptx.sreg.nctaid.x")
func __grid_shape_x()->u32;

@target("nvptx")
@intrinsic("llvm.nvvm.read.ptx.sreg.nctaid.y")
func __grid_shape_y()->u32;

@target("nvptx")
@intrinsic("llvm.nvvm.read.ptx.sreg.nctaid.z")
func __grid_shape_z()->u32;

@target("nvptx")
@intrinsic("llvm.nvvm.read.ptx.sreg.warpsize")
func __warp_size()->u32;

@target("nvptx")
func threadIndex()->Array<i64, 3> {
    return [__thread_idx_x().toi64(), __thread_idx_y().toi64(), __thread_idx_z().toi64()];
}

@target("nvptx")
func blockShape()->Array<i64, 3> {
    return [__block_shape_x().toi64(), __block_shape_y().toi64(), __block_shape_z().toi64()];
}

@target("nvptx")
func blockIndex()->Array<i64, 3> {
    return [__block_idx_x().toi64(), __block_idx_y().toi64(), __block_idx_z().toi64()];
}

@target("nvptx")
func gridShape()->Array<i64, 3> {
    return [__grid_shape_x().toi64(), __grid_shape_y().toi64(), __grid_shape_z().toi64()];
}
